---
title: "431 Class 21"
author: "Thomas E. Love"
date: "2017-11-09"
output:
  beamer_presentation:
    theme: "Madrid"
    fonttheme: "structurebold"
    colortheme: "whale"
    fig_caption: false
---

```{r set-options, echo=FALSE, cache=FALSE, message=FALSE}
knitr::opts_chunk$set(comment=NA)
options(width = 55)
```

## Today's Agenda

- Answer Sketches for the Airline Etiquette Exercises
- Two "Exciting" New Results worth a closer look
    + Stents 
    + Advil + Tylenol in the ED for acute pain
- A little more on the problems with *p* values
    + Statistical Significance doesn't have to be about *p* values
    + Confidence intervals as a partial solution journals like
    + Researcher Degrees of Freedom
    + The Garden of Forking Paths

## Today's R Setup

```{r setup, message = FALSE}
library(Epi); library(magrittr)
library(forcats); library(tidyverse)

fly <- fivethirtyeight::flying %>%
  select(id = respondent_id, recline_frequency, 
         recline_rude, unruly_child, 
         have_kids = children_under_18) %>%
  mutate(have_kids = factor(have_kids)) %>%
  filter(complete.cases(.))

source("Love-boost.R")
```

# Airplane Etiquette Survey: My Answers

## Airplane Etiquette Survey

https://fivethirtyeight.com/features/airplane-etiquette-recline-seat/

```{r}
summary(select(fly, unruly_child, have_kids, 
               recline_rude, recline_frequency))
```

## Exercise 1

1. Estimate a 90% confidence interval for the proportion of people answering either "Somewhat" or "Very" to the question of whether it is rude to knowingly bring an unruly child on a plane. What is the margin of error?

```{r}
fly %$% table(unruly_child) %>% addmargins
```

Our sample probability of ("Somewhat" or "Very") is (348 + 351) / 845 = 699 / 845 = 0.827. 

## Exercise 1 (continued)

We could use `binom.test` to calculate the 90% CI.

```{r}
prop.test(x = 699, n = 845, conf.level = 0.90)
```

## Exercise 1 (continued)

In fact, we know of at least three reasonable approaches.

Approach | 90% CI | half-width
--------: | -----------------: | ------------:
`prop.test` | (0.804, 0.848) | 0.022
`binom.test` | (0.804, 0.848) | 0.022
`saifs.ci` | (0.805, 0.849) | 0.022

In each case, the confidence interval's width is 0.044, and so the margin for error is approximately 0.022 (note that the confidence intervals we've fit aren't symmetric around the point estimate.)

## Exercise 2

2. Does the proportion of people who feel it is "Somewhat" or "Very" rude to knowingly bring an unruly child on a plane show a significant association with whether or not they themselves have children under 18 years of age?

```{r}
fly %$% table(have_kids, unruly_child) %>% addmargins
```

We'd like to rearrange this by collapsing the "Somewhat" and "Very" categories and moving the result left, and it might be nice to move "TRUE" to the top row, so as to approximate standard epidemiological format.

## Exercise 2 (data reshaping)

So, some data reshaping...

```{r}
fly1 <- fly %>% 
  mutate(kid_rude = 
           fct_collapse(unruly_child,
                        yes = c("Somewhat", "Very"),
                        no = "No"),
         kid_rude = fct_relevel(kid_rude, "yes"),
         have_kids = fct_relevel(have_kids,
                                 "TRUE"))
```

## Exercise 2 (revised table)

```{r}
fly1 %$% table(have_kids, kid_rude) %>% addmargins
```

Now, we apply the `twoby2` function from `Epi`...

```{r, eval = FALSE}
twoby2(fly1 %$% table(have_kids, kid_rude))
```

## Exercise 2 (`twoby2` results)

![](images/exercise2.png)

## Exercise 3

3. Given the actual data, what can you conclude about the true proportion of people who feel it is rude to recline your seat on a plane?

```{r}
fly %>% count(recline_rude)
```

It looks like 347 (279 + 68) respondents are in the "Somewhat" or "Very" category. That's 41.1% of the 845 respondents.

## Exercise 3 (SAIFS and other confidence intervals)

```{r}
saifs.ci(x = 347, n = 845)
```

The 95% CI from the `prop.test` and `binom.test` (without Bayesian augmentation) are also (0.377, 0.445)

## Exercise 4

4. Is there an association between how often you recline and your feelings about how rude it is?

```{r}
fly %$% table(recline_rude, recline_frequency) %>% addmargins
```

## Exercise 4 (graph)

```{r, echo = FALSE}
ggplot(fly, aes(x = recline_rude, fill = recline_rude)) +
  geom_bar() + guides(fill = FALSE) +
  facet_wrap(~ recline_frequency)
```

## Exercise 4 (initial chi-square test)

```{r}
fly %$% table(recline_rude, recline_frequency) %>% chisq.test
```

## Exercise 4 (collapsing the table)

```{r}
fly3 <- fly %>% 
 mutate(rude = 
  fct_collapse(recline_rude,
   "Somewhat or Very Rude" = c("Somewhat", "Very"),
   "Not Rude to Recline" = "No"),
  rude = fct_relevel(rude, "Somewhat or Very Rude"),
  behavior = fct_collapse(recline_frequency,
   "Usually or Always Recline" = c("Usually", "Always"),
   "Recline Half the time" = "About half the time",
   "Never or Rarely Recline" = 
     c("Never", "Once in a while")),
  behavior = fct_relevel(behavior, 
   "Usually or Always Recline", 
   "Recline Half the time"))
```

## Exercise 4 (graph, after collapsing)

```{r, echo = FALSE}
ggplot(fly3, aes(x = rude, fill = rude)) +
  geom_bar() + guides(fill = FALSE) +
  facet_wrap(~ behavior) + labs(x = "", y = "# of respondents")
```

## Exercise 4 (table, after collapsing)

```{r}
fly3 %$% table(behavior, rude) %>% addmargins
```

OK - we're ready for a chi-square test.

## Exercise 4 (chi-square test)

```{r}
fly3 %$% table(behavior, rude) %>% chisq.test
```

## Exercise 5

Suppose we wish to estimate the power a study will have to estimate the difference in proportion of people who feel that waking someone up to go for a walk is very or somewhat rude, comparing taller people to shorter people. Suppose we propose a new study, where we will collect data from 1200 tall and 1200 short people, and we look to declare as important any observed difference where one group is at 73% or more, while the other is at 70% or less. 

5. Using a 10% significance level, what power will we have? 

```{r, echo = FALSE}
power.prop.test(n = 1200, p1 = 0.70, p2 = 0.73, 
                sig.level = 0.10)
```

Result on next slide.

## Exercise 5 Result

```{r}
power.prop.test(n = 1200, p1 = 0.70, p2 = 0.73, 
                sig.level = 0.10)
```


## Exercise 6

6. To obtain at least 80% power, how big a sample would we need?

```{r}
power.prop.test(p1 = 0.70, p2 = 0.73, 
                sig.level = 0.10, power = 0.80)
```

# What's in the news?

## Al-Lamee R et al. ORBITA: A Double-blind RCT of Cardiac Stents (*The Lancet* 2017-11-02)

![](images/stent_00.png)

## Al-Lamee R et al *Lancet* 2017-11-02

![](images/stent1.png)

## *New York Times* (2017-11-02) by Gina Kolata

![](images/stent2.png)

## Al-Lamee R et al *Lancet* 2017-11-02

![](images/stent_lancet_2.png)

## Al-Lamee R et al *Lancet* 2017-11-02

**Primary (pre-specified) endpoint**: difference between PCI and placebo groups in the change in treadmill exercise time.

**Design parameters**: anticipated an effect size of 30 seconds (less than what you'd get from a single antianginal medication), and assumed a between-patient standard deviation of change in exercise time of 75 seconds. 

What is the **power** of a sample of, say, 100 patients per group to detect such a difference between the PCI and placebo groups at the 5% significance level, with a two-sided test? 

## ORBITA trial power?

```{r}
power.t.test(n = 100, delta = 30, sd = 75, sig.level = 0.05)
```

## Al-Lamee R et al *Lancet* 2017-11-02

![](images/stent_lancet_1.png)

## ORBITA primary result?

- n = 105 randomized to PCI and 95 randomized to placebo (ITT).
- Observed difference between the groups was 16.6 (95% CI -8.9 - 42.0)

![](images/stent_lancet_3.png)

## A Secondary Outcome (CCS angina grade)

![](images/stent_lancet_4.png)

## $\chi^2$ for Pre-randomization vs. follow-up

```{r}
ccs <- matrix(c(51, 27, 27, 50, 22, 19), 
              byrow=TRUE, nrow = 2, ncol = 3)
rownames(ccs) <- c("PCI", "Placebo")
colnames(ccs) <- c("No change", "Improved 1 class", 
                   "Improved 2+")
```

```
> ccs

        No change Improved 1 class Improved 2+
PCI            51               27          27
Placebo        50               22          19

> chisq.test(ccs)

Pearson's Chi-squared test data:  ccs
X-squared = 0.91608, df = 2, p-value = 0.6325
```
## *New York Times* (2017-11-02) by Gina Kolata

![](images/stent3.png)

## Chang AK et al. Opioids vs. Non-Opioids for Acute Extremity Pain in the ED (JAMA 2017-11-07)

![](images/opioid_00.png)

## Advil vs. Opioids for acute pain in ED (JAMA 2017-11-07)

![](images/opioid1.png)

## Chang et al. Abstract

![](images/opi02.png)

## Chang et al. Sample Size

![](images/opi_n.png)

## Chang et al. Results

![](images/opi_results.png)

## Chang et al. Abstract

![](images/opi03.png)

## *New York Times* (2017-11-08) by Nicholas Bakalar

Researchers studied 416 men and women who arrived in the E.R. with moderate to severe pain in their arms or legs from sprains, strains, fractures or other injuries. They randomly assigned them to an oral dose of acetaminophen (Tylenol) with either ibuprofen (Advil) or the opioids oxycodone, hydrocodone or codeine. 

Two hours later, they questioned them using an 11-point pain scale (higher scores = more pain).

- The average score was 8.7 before taking medicine. 
- That score decreased by:
    + 4.3 points with ibuprofen and Tylenol, 
    + 4.4 with oxycodone and Tylenol, 
    + 3.5 with hydrocodone and Tylenol, and 
    + 3.9 with codeine and Tylenol. 

In other words, there was no significant difference, either statistically or clinically, among any of the four regimens. 


# On p values

## George Cobb's Questions (with Answers)

In February 2014, George Cobb, Professor Emeritus of Mathematics and Statistics at Mount Holyoke College, posed these questions to an ASA discussion forum:

>- Q: Why do so many colleges and grad schools teach *p* = 0.05?

>- A: Because that's still what the scientific community and journal editors use.

>- Q: Why do so many people still use *p* = 0.05?

>- A: Because that's what they were taught in college or grad school.

## Now what?

![](images/bear.jpg)

So sad...

## Gelman on Statistical Significance

"... we use the term statistically significant in the conventional way, to mean that an estimate is **at least two standard errors away** from some "null hypothesis" or prespecified value that would indicate no effect present. An estimate is statistically insignificant if the observed value could reasonably be explained by simple chance variation, much in the way that a sequence of 20 coin tosses might happen to come up 8 heads and 12 tails; we would say that this result is not statistically significantly different from chance. More precisely, the observed proportion of heads is 40 percent but with a standard error of 11 percent - thus, the data are less than two standard errors away from the null hypothesis of 50 percent, and the outcome could clearly have occurred by chance. Standard error is a measure of the variation in an estimate and gets smaller as a sample size gets larger, converging on zero as the sample increases in size."

[Gelman's blog (2017-10-28)](http://andrewgelman.com/2017/10/28/favorite-definition-statistical-significance/)

## The Value of a *p*-Valueless Paper

Jason T. Connor (2004) *American J of Gastroenterology* 99(9): 1638-40.

Abstract: As is common in current bio-medical research, about 85% of original contributions in *The American Journal of Gastroenterology* in 2004 have reported *p*-values. However, none are reported in this issue's article by Abraham et al. who, instead, rely exclusively on effect size estimates and associated confidence intervals to summarize their findings. **Authors using confidence intervals communicate much more information in a clear and efficient manner than those using** *p*-**values. This strategy also prevents readers from drawing erroneous conclusions caused by common misunderstandings about** *p*-**values**. I outline how standard, two-sided confidence intervals can be used to measure whether two treatments differ or test whether they are clinically equivalent.

DOI: 10.1111/j.1572-0241.2004.40592.x

## Why Dividing Data Comparisons into Categories based on Significance Levels is Terrible.

> The common practice of dividing data comparisons into categories based on significance levels is terrible, but it happens all the time.... so it's worth examining the prevalence of this error.

[Link to Andrew Gelman's blog, 2016-10-15](http://andrewgelman.com/2016/10/15/marginally-significant-effects-as-evidence-for-hypotheses-changing-attitudes-over-four-decades/)

## Gelman on *p* values, 1

Let me first briefly explain why categorizing based on p-values is such a bad idea. Consider, for example, this division: 

- "really significant" for *p* < .01, 
- "significant" for *p* < .05, 
- "marginally significant" for *p* < .1, and 
- "not at all significant" otherwise. 

Now consider some typical *p*-values in these ranges: say, *p* = .005, *p* = .03, *p* = .08, and *p* = .2. 

Translate these two-sided *p*-values back into z-scores, which we can do in R via `qnorm(c(.005, .03, .08, .2)/2, lower.tail = FALSE)`

## Gelman on *p* values, 2

Description | really sig. | sig. | marginally sig.| not at all sig.
---------: | ----: | ----: | ----: | ----:
*p* value | 0.005 | 0.03 | 0.08 | 0.20
Z score | 2.8 | 2.2 | 1.8 | 1.3

The seemingly yawning gap in p-values comparing the not at all significant *p*-value of .2 to the really significant *p*-value of .005, is only 1.5. 

If you had two independent experiments with z-scores of 2.8 and 1.3 and with equal standard errors and you wanted to compare them, you'd get a difference of 1.5 with a standard error of 1.4, which is completely consistent with noise.


## Gelman on *p* values, 3

**The key point**: The difference between statistically significant and NOT statistically significant is not, generally, statistically significant.

From a **statistical** point of view, the trouble with using the p-value as a data summary is that the p-value is only interpretable in the context of the null hypothesis of zero effect, and (much of the time), nobody's interested in the null hypothesis. Indeed, once you see comparisons between large, marginal, and small effects, the null hypothesis is irrelevant, as you want to be comparing effect sizes.

From a **psychological** point of view, the trouble with using the p-value as a data summary is that this is a kind of deterministic thinking, an attempt to convert real uncertainty into firm statements that are just not possible (or, as we would say now, just not replicable).

# p Hacking and "Researcher Degrees of Freedom"

## Hack Your Way To Scientific Glory

https://fivethirtyeight.com/features/science-isnt-broken

![](images/hacking.png)


## What can you get?

I was able to get 

- *p* < 0.01 (positive effect of Democrats on economy)
- *p* = 0.01 (negative effect of Democrats)
- *p* = 0.03 (negative effect of Democrats)
- *p* = 0.03 (positive effect of Democrats)

but also ...

- *p* = 0.05, 0.06, 0.07, 0.09, 0.17, 0.19, 0.20, 0.22, 0.23, 0.47, 0.51

without even switching parties, exclusively by changing my definitions of terms (section 2 of the graphic.)

## "Researcher Degrees of Freedom", 1

> [I]t is unacceptably easy to publish "statistically significant" evidence consistent with any hypothesis.

> The culprit is a construct we refer to as **researcher degrees of freedom**. In the course of collecting and analyzing data, researchers have many decisions to make: Should more data be collected? Should some observations be excluded? Which conditions should be combined and which ones compared? Which control variables should be considered? Should specific measures be combined or transformed or both?

Simmons et al. [$\textcolor{blue}{link}$](http://journals.sagepub.com/doi/abs/10.1177/0956797611417632) 

## "Researcher Degrees of Freedom", 2

> ... It is rare, and sometimes impractical, for researchers to make all these decisions beforehand. Rather, it is common (and accepted practice) for researchers to explore various analytic alternatives, to search for a combination that yields statistical significance, and to then report only what worked. The problem, of course, is that the likelihood of at least one (of many) analyses producing a falsely positive finding at the 5% level is necessarily greater than 5%.

For more, see 

- Gelman's blog [$\textcolor{blue}{2012-11-01}$](http://andrewgelman.com/2012/11/01/researcher-degrees-of-freedom/) "Researcher Degrees of Freedom", 
- Paper by [$\textcolor{blue}{Simmons}$](http://journals.sagepub.com/doi/abs/10.1177/0956797611417632) and others, defining the term.

## And this is really hard to deal with...

**The garden of forking paths**: Why multiple comparisons can be a problem, even when there is no fishing expedition or p-hacking and the research hypothesis was posited ahead of time

> Researcher degrees of freedom can lead to a multiple comparisons problem, even in settings
where researchers perform only a single analysis on their data. The problem is there can be a
large number of potential comparisons when the details of data analysis are highly contingent on
data, without the researcher having to perform any conscious procedure of fishing or examining
multiple p-values. We discuss in the context of several examples of published papers where
data-analysis decisions were theoretically-motivated based on previous literature, but where the
details of data selection and analysis were not pre-specified and, as a result, were contingent on
data.

- [$\textcolor{blue}{Link}$](http://www.stat.columbia.edu/~gelman/research/unpublished/p_hacking.pdf) to the paper from Gelman and Loken

## Benjamin et al 2017 Redefine Statistical Significance

We propose to change the default P-value threshold for statistical significance for claims of new discoveries from 0.05 to 0.005.

- 0.005 is stringent enough to "break" the current system - makes it very difficult for researchers to reach threshold with noisy, useless studies.

Visit the main [$\textcolor{blue}{article}$](https://psyarxiv.com/mky9j/). Visit an explanatory piece in [$\textcolor{blue}{Science}$](http://www.sciencemag.org/news/2017/07/it-will-be-much-harder-call-new-findings-significant-if-team-gets-its-way).

### Lakens et al. Justify Your Alpha

"In response to recommendations to redefine statistical significance to $p \leq .005$, we propose that researchers should transparently report and justify all choices they make when designing a study, including the alpha level." Visit [$\textcolor{blue}{link}$](https://psyarxiv.com/9s3y6).

## Quiz 2 Setup

- Quiz 2 will be yours by 5 PM today. 
    + It's now due Tuesday Nov 14 at **8 AM**.
