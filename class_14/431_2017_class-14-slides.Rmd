---
title: "431 Class 14"
author: "Thomas E. Love"
date: "2017-10-12"
output:
  beamer_presentation:
    theme: "Madrid"
    fonttheme: "structurebold"
    colortheme: "whale"
    fig_caption: false
---

```{r set-options, echo=FALSE, cache=FALSE, message=FALSE}
knitr::opts_chunk$set(comment=NA)
options(width = 55)
```

## Today's R Setup

```{r setup, message = FALSE}
library(boot); library(broom); library(magrittr)
library(tidyverse)

source("Love-boost.R")
```

## Today's Agenda

1. Project Task B Group Meetings
2. Statistical Inference and the `dm192` data
    + Hypothesis Testing and P Values
    + Comparing Two Population Means using Paired Samples
    + Comparing Two Population Means using Indepdendent Samples

Project Task A is due on Friday 2017-10-13 at noon.

## Project Task B Group Meetings

Go.

## Description of the `dm192` data

I stored the `dm192.csv` data in a subdirectory of my class 14 project directory called `data`.

```{r load_dm192}
dm192 <- read.csv("data/dm192.csv") %>% tbl_df
head(dm192,5) # show just the first 5 rows
```

## The Large Sample Formula for the CI around $\mu$

The two-tailed 100(1-$\alpha$)% confidence interval for a population mean $\mu$ (based on the Normal distribution) is:

- The Lower Bound is $\bar{x} - Z_{\alpha/2}(\sigma / \sqrt{n})$ and the Upper Bound is $\bar{x} + Z_{\alpha/2}(\sigma / \sqrt{n})$

where $Z_{\alpha/2}$ is the value that cuts off the top $\alpha/2$ percent of the standard Normal distribution (the Normal distribution with mean 0 and standard deviation 1). 

## Obtaining the $Z_{\alpha/2}$ value using `qnorm`

We can obtain this cutoff value from R by substituting in the desired proportion for alphaover2 into the `qnorm` function as follows:

`qnorm(alphaover2, lower.tail=FALSE)`

For example, if we are building a 95% confidence interval, we have 100(1-$\alpha$) = 95, so that $\alpha$ is 0.05, or 5%. This means that the cutoff value we need to find is $Z_{0.05/2} = Z_{.025}$, and this turns out to be 1.96.

```{r z_for_95CI}
qnorm(0.025, lower.tail=FALSE)
```

## Commonly Used Cutoffs based on the Normal Distribution

- If we're building a two-tailed 95% confidence interval, we'll use $Z_{.025}$ = 1.96
- For a two-tailed 90% confidence interval, we use $Z_{.05}$ = 1.645
- For a two-tailed 99% confidence interval, we use $Z_{.005}$ = 2.576
- For a two-tailed 50% confidence interval, we use $Z_{.25}$ = 0.67
- For a two-tailed 68% confidence interval, we use $Z_{.16}$ = 0.99

## Lots of CIs use the Normal distribution

- The usual 95% confidence interval for large samples is an estimate $\pm$ 2 standard errors\footnote{The use of 2 standard errors for a confidence interval for a population mean is certainly reasonable whenever n is 60 or more. This is because the t distribution with 59 degrees of freedom has a 0.025 cutoff of 2.0, anyway.}.
- Also, from the Normal distribution, an estimate $\pm$ 1 standard error is a 68% confidence interval, and an estimate $\pm$ 2/3 of a standard error is a 50% confidence interval. 
- A 50% interval is particularly easy to interpret because the true value should be inside the interval about as often as it is not. 
- A 95% interval is thus about three times as wide as a 50% interval. 
- In general, the larger the confidence required, the wider the interval will need to be.

## Large-Sample CI for Systolic BP Mean, $\mu$

Since we have a fairly large sample (*n* = `r length(dm192$sbp)`), we could consider using a large-sample approach (assuming the sample standard deviation is equal to the population standard deviation, and then using the Normal distribution) to estimate a confidence interval for the mean systolic blood pressure in the population of all adults with diabetes who live in Northeast Ohio. The 95% confidence interval is calculated as $\bar{x} \pm Z_{\alpha/2}(\sigma / \sqrt{n})$, and here we will assume that $s = \sigma$ which may be reasonable with a fairly large sample size. 

- We have $n$ = `r length(dm192$sbp)` observations, and since we want a 95% confidence interval, $\alpha$ = 0.05
- Our sample mean $\bar{x}$ = `r round(mean(dm192$sbp),2)` and standard deviation $s$ = `r round(sd(dm192$sbp),2)`
- So the standard error is `r round( sd(dm192$sbp)/ sqrt(length(dm192$sbp)), 2)`

The 95% CI is thus `r round(mean(dm192$sbp),2)` $\pm$ 1.96(`r round( sd(dm192$sbp)/ sqrt(length(dm192$sbp)), 2)`), or (`r round(mean(dm192$sbp),2) - round(1.96*sd(dm192$sbp)/ sqrt(length(dm192$sbp)), 2)`, `r round(mean(dm192$sbp),2) + round(1.96*sd(dm192$sbp)/ sqrt(length(dm192$sbp)), 2)`) using the Normal distribution. 

- Our 95% CI based on the t distribution was (`r round(t.test(dm192$sbp, conf.level = 0.95)$conf[c(1,2)],1)`).

## Assumptions of a t-based Confidence Interval

> "Begin challenging your assumptions. Your assumptions are your windows on the world. Scrub them off every once in awhile or the light won't come in." (Alan Alda)

1. Sample is drawn at random from the population or process.
2. Samples are drawn independently from each other from a population or process whose distribution is unchanged during the sampling process.
3. Population or process follows a Normal distribution.

### Can we drop any of these assumptions?

Only if we're willing to consider alternative inference methods.

## What is a Bootstrap and Why Should I Care?

The bootstrap (and in particular, what's known as bootstrap resampling) is a really good idea that you should know a little bit about\footnote{See Good PI Hardin JW Common Errors in Statistics -- a very helpful book.}. 

If we want to know how accurately a sample mean estimates the population mean, we would ideally like to take a very, very large sample, because if we did so, we could conclude with something that would eventually approach mathematical certainty that the sample mean would be very close to the population mean. 

But we can rarely draw enormous samples. So what can we do?  

## Resampling is A Big Idea

If we want our sample mean to accurately estimate the population mean, we would ideally like to take a very, very large sample, so as to get very precise estimates. But we can rarely draw enormous samples. So what can we do?  

Oversimplifying, the idea is that if we sample (with replacement) from our current data, we can draw a new sample of the same size as our original. 

- And if we repeat this many times, we can generate as many samples of, say, 192 systolic blood pressures, as we like. 
- Then we take these thousands of samples and calculate (for instance) the sample mean for each, and plot a histogram of those means. 
- If we then cut off the top and bottom 5% of these sample means, we obtain a reasonable 90% confidence interval for the population mean. 

## Bootstrap: Estimating a confidence interval for $\mu$

What the computer does:

1. Resample the data with replacement, until it obtains a new sample that is equal in size to the original data set. 
2. Calculates the statistic of interest (here, a sample mean.) 
3. Repeat the steps above many times (the default is 1,000 using our approach) to obtain a set of 1,000 sample means. 
4. Sort those 1,000 sample means in order, and estimate the 90% confidence interval for the population mean based on the middle 90% of the 1,000 bootstrap samples.
5. Send us a result, containing the sample mean, and a 90% confidence interval for the population mean

## When is a Bootstrap Confidence Interval for $\mu$ Reasonable?

The interval will be reasonable as long as we are willing to believe that:

- the original sample was a random sample (or at least a completely representative sample) from a population, 
- and that the samples are independent of each other 
- and that the samples are identically distributed (even though that distribution may not be Normal.) 

A downside is that you and I will get (somewhat) different answers if we resample from the same data.

## 90% CI for population mean $\mu$ using bootstrap

The command that we use to obtain a CI for $\mu$ using the basic nonparametric bootstrap and without assuming a Normally distributed population, is `smean.cl.boot`, a part of the `Hmisc` package in R.

```{r boot_sbp, message=FALSE}
set.seed(43101) 
Hmisc::smean.cl.boot(dm192$sbp, conf = 0.90)
```

## Comparing Bootstrap and T-Based Confidence Intervals

- The `smean.cl.boot` function (unlike most R functions) deletes missing data automatically, as does the `smean.cl.normal` function, which produces the t-based confidence interval.

```{r boot_sbp_2}
Hmisc::smean.cl.boot(dm192$sbp, conf = 0.90)
Hmisc::smean.cl.normal(dm192$sbp, conf = 0.90)
```

## Rerunning 90% CI for $\mu$ via Bootstrap

```{r boot_sbp_3}
set.seed(43102); Hmisc::smean.cl.boot(dm192$sbp, conf = 0.9)
set.seed(43103); Hmisc::smean.cl.boot(dm192$sbp, conf = 0.9)
set.seed(43104) 
  Hmisc::smean.cl.boot(dm192$sbp, conf = 0.9, B = 2000)
```

## Bootstrap: Changing the Confidence Level

```{r boot_sbp_4}
set.seed(43105); Hmisc::smean.cl.boot(dm192$sbp, conf = 0.90)
set.seed(43106); Hmisc::smean.cl.boot(dm192$sbp, conf = 0.95)
set.seed(43107); Hmisc::smean.cl.boot(dm192$sbp, conf = 0.99)
```

## Bootstrap for a One-Sided Confidence Interval

If you want to estimate a one-sided confidence interval for the population mean using the bootstrap, then the procedure is as follows:

1. Determine $\alpha$, the significance level you want to use in your one-sided confidence interval. Remember that $\alpha$ is 1 minus the confidence level. Let's assume we want a 90% one-sided interval, so $\alpha$ = 0.10.
2. Double $\alpha$ to determine the significance level we will use in the next step to fit a two-sided confidence interval.
3. Fit a two-sided confidence interval with confidence level $100(1 - 2\alpha)$. Let the bounds of this interval be (*a*, *b*).
4. The one-sided (greater than) confidence interval will have *a* as its lower bound.
5. The one-sided (less than) confidence interval will have *b* as its upper bound.

## One-sided CI for $\mu$ via the Bootstrap

Suppose that we want to find a 90% one-sided upper bound for the population mean systolic blood pressure among Northeast Ohio adults with diabetes, $\mu$, using the bootstrap. 

Since we want a 90% confidence interval, we have $\alpha$ = 0.10. We double that to get $\alpha$ = 0.20, which implies we need to instead fit a two-sided 80% confidence interval.

```{r 80_pct_CI_for_sbp_with_bootstrap}
set.seed(43108); Hmisc::smean.cl.boot(dm192$sbp, conf = 0.80)
```

Since the upper bound of this two-sided 80% CI is 135.77, that will also be the upper bound for a 90% one-sided CI.

## Additional Notes on the Bootstrap

Bootstrap resampling confidence intervals do not follow the general confidence interval strategy using a point estimate $\pm$ a margin for error. 

- A bootstrap interval is often asymmetric, and while it will generally have the point estimate (the sample mean) near its center, for highly skewed data, this will not necessarily be the case.

- I usually use either 1,000 (the default) or 10,000 bootstrap replications for building confidence intervals - practically, it makes little difference.

The bootstrap may seem like the solution to all problems in theory, we could use the same approach to find a confidence interval for any other statistic -- it's not perfect, but it is very useful. 

- It does eliminate the need to worry about the Normality assumption in small sample size settings, but it still requires independent and identically distributed samples.

## Bootstrap Resampling: Advantages and Caveats

Bootstrap procedures exist for virtually any statistical comparison - the t-test analog above is just one many possibilities, and bootstrap methods are rapidly gaining on more traditional approaches in the literature thanks mostly to faster computers.

The bootstrap produces clean and robust inferences (such as confidence intervals) in many tricky situations. 

It is still possible that the results can be both:

- **inaccurate** (i.e. they can, include the true value of the unknown population mean less often than the stated confidence probability) and 
- **imprecise** (i.e., they can include more extraneous values of the unknown population mean than is desirable).

## Bootstrap CI for the Population Median, Step 1

If we are willing to do a small amount of programming work in R, we can obtain bootstrap confidence intervals for other population parameters besides the mean. One statistic of common interest is the median. How do we find a confidence interval for the population median using a bootstrap approach? Use the `boot` package, as follows.

In step 1, we specify a new function to capture the medians from our sample. 

```{r boot_median_step1}
f.median <- function(y, id) 
{    median ( y[id])  }
```

## Bootstrap CI for the Population Median, Step 2

In step 2, we summon the `boot` package and call the `boot.ci` function:

```{r boot_median_step2, message=FALSE}
set.seed(431787)
boot.ci(boot (dm192$sbp, f.median, 1000), 
        conf=0.90, type="basic")
```

## Bootstrap CI for the Population Median vs. Mean

- Note that the sample **median** of the SBP data is `r median(dm192$sbp)` mm Hg.

- Our 90% confidence interval for the population **median** SBP among NE Ohio adults with diabetes is (`r set.seed(431787); boot.ci(boot (dm192$sbp, f.median, 1000), conf=0.90, type="basic")$basic[4]`, `r set.seed(431787); boot.ci(boot (dm192$sbp, f.median, 1000), conf=0.90, type="basic")$basic[5]`) according to the bootstrap, using the random seed `431787`. 


- The sample **mean** of the SBP data is `r round(mean(dm192$sbp),1)` mm Hg.

- The 90% bootstrap CI for the population **mean** SBP, $\mu$, is (`r set.seed(43121); round(Hmisc::smean.cl.boot(dm192$sbp, conf = 0.90)[2],1)`, `r set.seed(43121); round(Hmisc::smean.cl.boot(dm192$sbp, conf = 0.90)[3],1)`) if we use the random seed `43121`.

## The Wilcoxon Signed Rank Procedure for CIs

It turns out to be difficult to estimate an appropriate confidence interval for the median of a population, which might be an appealing thing to do, particularly if the sample data are clearly not Normally distributed, so that a median seems like a better summary of the center of the data. Bootstrap procedures are available to perform the task.

The Wilcoxon signed rank approach can be used as an alternative to t-based procedures to build interval estimates for the population *pseudo-median* when the population cannot be assumed to follow a Normal distribution. 

As it turns out, if you're willing to assume the population is **symmetric** (but not necessarily Normally distributed) then the pseudo-median is actually equal to the population median.

## What is a Pseudo-Median?

The pseudo-median of a particular distribution G is the median of the distribution of (u + v)/2, where both u and v have the same distribution (G). 

- If the distribution G is symmetric, then the pseudomedian is equal to the median. 
- If the distribution is skewed, then the pseudomedian is not the same as the median. 
- For any sample, the pseudomedian is defined as the median of all of the midpoints of pairs of observations in the sample. 

## Getting the Wilcoxon Signed Rank-based CI in R

```{r wilcoxon_sbp_1}
wilcox.test(dm192$sbp, conf.int=TRUE, conf.level=0.95)
```

## Interpreting the Wilcoxon CI for the Population Median

If we're willing to believe the `sbp` values come from a population with a symmetric distribution, the 95% Confidence Interval for the population median would be (`r round(wilcox.test(dm192$sbp, conf.int=TRUE, conf.level=0.95)$conf.int,1)`)

For a non-symmetric population, this only applies to the *pseudo-median*.

Note that the pseudo-median (133.5) is actually fairly close in this situation to the sample mean (134.2) as well as to the sample median (133), as it usually will be if the population actually follows a symmetric distribution, as the Wilcoxon approach assumes.


## Comparing Population Means via Paired Samples

The `dm192` data has current systolic blood pressure (`sbp`), and systolic blood pressure from last year (`sbp_old`). Suppose we want to describe the mean SBP change in not just our sample, but instead the entire **population** (adults who live in NE Ohio with diabetes) over the past year.

```{r get key elements of dm192}
dm_first <- select(dm192, pt.id, sbp, sbp_old)
summary(dm_first)
```

## Each subject provides both a `sbp_old` and `sbp`

```{r scatterplot of dmsample, echo = FALSE}
ggplot(dm_first, aes(x = sbp_old, y = sbp)) + 
    geom_point() +
    theme_bw() +
    labs(title = "SBP for this year and last year in each of 192 subjects",
         x = "sbp_old = Last Year's Systolic BP (mm Hg)",
         y = "sbp = This Year's Systolic BP (mm Hg)")
```

## The Impact of Pairing

```{r scatterplot of dmsample with annotations, echo = FALSE}
ggplot(dm_first, aes(x = sbp_old, y = sbp)) + 
    geom_point() +
    annotate("point", x = 197, y = 200, col = "blue", size = 2) +
    annotate("point", x = 120, y = 107, col = "red", size = 2) +
    annotate("text", x = 190, y = 195, label = "Patient 53", col = "blue", size = 5) +
    annotate("text", x = 130, y = 107, label = "Patient 148", col = "red", size = 5) +
    annotate("text", x = 120, y = 180, col = "purple", size = 5,
             label = paste("Pearson r = ", round(cor(dm_first$sbp, dm_first$sbp_old),2))) +
    theme_bw() + 
    labs(title = "SBP for this year and last year in each of 192 subjects",
         x = "sbp_old = Last Year's Systolic BP (mm Hg)",
         y = "sbp = This Year's Systolic BP (mm Hg)")
```

## A Matched Samples Plot ("After - Before" Plot)

Each subject provides both a value for `sbp` and one for `sbp_old`:

```{r before-after plot, echo = FALSE, fig.height = 5}
## first re-express the data in a frame with pt.id
## time and systolicbp
dm_firstlong <- gather(dm_first, key = time, value = SBP, sbp_old, sbp)

ggplot(dm_firstlong, aes(x = time, y = SBP, group = pt.id)) + 
    geom_point(aes(col = pt.id)) +
    geom_line(aes(col = pt.id)) + 
    labs(title = "Matched Samples Plot for SBP in dm192",
         y = "Systolic BP (mm Hg)", x = "")
```

Patient 53 is the patient on top, with `sbp` = 200, and `sbp_old` = 197.

## Paired Samples? Calculate Paired Differences

```{r calculate paired diffs for dmsample}
dm_first$diffs <- dm_first$sbp - dm_first$sbp_old; 
dm_first[1:3,]
mosaic::favstats(dm_first$diffs)
```

## EDA for the Paired Differences

```{r histogram boxplot and normal Q-Q plot of sbp diffs, echo = FALSE, message = FALSE}
p1 <- ggplot(dm_first, aes(x = diffs)) +
    geom_histogram(aes(y = ..density..), bins = fd_bins(dm_first$diffs),
                 fill = "#0A304E", col = "white") +
    coord_flip() +
    stat_function(fun = dnorm,
                args = list(mean = mean(dm_first$diffs), 
                            sd = sd(dm_first$diffs)),
                lwd = 1.5, col = "navy") +
    labs(title = "Histogram",
       x = "Change in Systolic BP", y = "Density")

p2 <- ggplot(dm_first, aes(x = 1, y = diffs)) +
  geom_boxplot(fill = "#0A304E", notch = TRUE, col = "navy", outlier.color = "#0A304E") +
  theme(axis.text.x = element_blank(), axis.ticks.x = element_blank()) +
  labs(title = "Boxplot",
       y = "", x = "")

p3 <- ggplot(dm_first, aes(sample = diffs)) +
  geom_qq(col = "#0A304E", size = 2) +
  geom_abline(intercept = qq_int(dm_first$diffs), 
              slope = qq_slope(dm_first$diffs),
              col = "navy") +
  labs(title = "Normal Q-Q",
       y = "", x = "")

gridExtra::grid.arrange(p1, p2, p3, nrow=1, 
   top = "Change in Systolic BP in mm Hg (This Year minus Last Year)")
```

## t test for the Paired Differences

```{r t test for paired diffs}
t.test(dm_first$sbp, dm_first$sbp_old, paired = TRUE)
```

## Five Steps to Complete a Hypothesis Test

1.	Specify the null hypothesis, $H_0$ (which usually indicates that there is no difference between various groups of subjects)
2.	Specify the research or alternative hypothesis, $H_1$, sometimes called $H_A$ (which usually indicates that there is some difference or some association between the results in those same groups of subjects).
3.	Specify the test procedure or test statistic to be used to make inferences to the population based on sample data. 
    - Here we specify $\alpha$, the probability of incorrectly rejecting $H_0$ that we are willing to accept. Often, we use $\alpha = 0.05$
4.	Obtain the data, and summarize it to obtain a relevant test statistic, and a resulting $p$ value.
5.	Use the $p$ value to either
    - **reject** $H_0$ in favor of the alternative $H_A$ (concluding that there is a statistically significant difference/association at the $\alpha$ significance level) 
    - or **retain** $H_0$ (and conclude that there is no statistically significant difference/association at the $\alpha$ significance level)

## Step 1. The Null Hypothesis

- A null hypothesis is a statement about a population parameter, and it describes the current state of knowledge -- the status quo -- or our model for the world before the research is undertaken and data are collected. 
- It often specifies an idea like "no difference" or "no association" in testable statistical terms.

## The Null Hypothesis in the SBP in Diabetes Study

- Here, our null hypothesis will refer to the population mean of the paired differences in systolic blood pressure (in mm Hg) comparing the same subjects last year vs. this year.

- $H_0$: Population Mean SBP This Year = Population Mean SBP Last Year
    - If there is in fact no difference between the years, then the this year -- last year difference will be zero.
- Symbolically, $H_0$: $\mu_d$ = 0, where $\mu_d$ is the population mean (this year -- last year) difference in systolic BP. 
    + Of course, we've built confidence intervals for means like this already.

## Step 2. The Alternative Hypothesis

- The alternative or research hypothesis, $H_A$, is in some sense the opposite of the null hypothesis. 
- It specifies the values of the population parameter that are not part of $H_0$. 
- If $H_0$ implies "no difference", then $H_A$ implies that "there is a difference". 

## The Alternative Hypothesis in the SBP in Diabetes Study

Since our null hypothesis is

$H_0$: Population Mean SBP This Year -- Population Mean SBP Last Year = 0, or $H_0: \mu_d = 0$,

our alternative hypothesis will therefore cover all other possibilities:

$H_A$: Population Mean SBP This Year -- Population Mean SBP Last Year $\neq$ 0, or $H_A: \mu_d \neq 0$.

Occasionally, we'll use a one-sided alternative, like $H_A: \mu_d < 0$, in which case, $H_0: \mu_d \geq 0$. 

## Step 3: The Test Procedure and Assumptions

We want to compare the population mean of the paired differences, $\mu_d$, to a fixed value, 0. 

We must be willing to believe that the paired differences data are a random (or failing that, representative) sample from the population of interest, and that the samples were drawn independently, from an identical population distribution. 

Given those assumptions, we have four possible strategies to complete our paired samples comparison:

## The Four Strategies for Testing Paired Differences

a. Assume the paired differences come from a Normally distributed population, and perform a **one-sample t test** on the paired differences, and use the resulting *p* value to draw a conclusion about the relative merits of $H_0$ and $H_A$.
b. Or perform a **Wilcoxon signed-rank test** on the paired differences, which would be more appropriate than the t test if the population of paired differences was not Normally distributed, but was reasonably symmetric, and use the resulting *p* value.
c. Or develop a **bootstrap confidence interval** for the population mean of the paired differences, as we've done in the past. This wouldn't require an assumption about Normality. We'd then use that confidence interval to assess the relative merits of $H_0$ and $H_A$.

I'm skipping the **sign test**. See the Part B notes.

## Step 4: Collect and summarize the data, usually with a *p* value

Of course, in this case, we've already gathered the data. The task now is to obtain and interpret the tests using each of the four procedures listed previously. The main task we will leave to the computer is the calculation of a **p value**.

### Defining a *p* Value

The *p* value assumes that the null hypothesis is true, and estimates the probability, under those conditions (i.e. $H_0$ is true), that we would obtain a result as much in favor or more in favor of the alternative hypothesis $H_A$ as we did. 

- The *p* value is a conditional probability of seeing evidence as strong or stronger in favor of $H_A$ calculated assuming that $H_0$ is true.

## Using the *p* Value

The way we use the *p* value is to compare it to $\alpha$, our pre-specified tolerance level for a certain type of error (Type I error, specifically -- rejecting $H_0$ when it is in fact true.) 

- If the *p* value is less than $\alpha$, we will reject $H_0$ in favor of $H_A$
- If the *p* value is greater than or equal to $\alpha$, we will retain $H_0$.

## t Test for the SBP in Diabetes Study

```{r t test again for sbp}
t.test(dm_first$sbp-dm_first$sbp_old)
```

The alternative hypothesis is `true difference in means is not equal to 0.` Should we retain or reject $H_0$ at $\alpha = 0.05$?

## Wilcoxon Signed Rank for the SBP in Diabetes data

```{r wilcoxon signed rank test for twins data}
wilcox.test(dm_first$sbp - dm_first$sbp_old, conf.int=TRUE)
```

Should we reject or retain $H_0: \mu_d = 0$ based on this test?

## What The *p* Value isn't

The *p* value is not a lot of things. It's **NOT**

- The probability that the alternative hypothesis is true
- The probability that the null hypothesis is false
- Or anything like that.

The *p* value **IS** a statement about the amount of statistical evidence contained in the data that favors the alternative hypothesis $H_A$. It's a measure of the evidence's credibility.

## Bootstrap CI for the Twins data

Using a significance level of $\alpha$ = 0.05 is equivalent to using a confidence level of 100(1-$\alpha$)% = 95%:

```{r bootstrap with set seed for twins}
set.seed(4311); Hmisc::smean.cl.boot(dm_first$diffs) 
```

So, according to this confidence interval, a reasonable range (with 95% confidence) for $\mu$, the population mean of the unadjusted -- adjusted differences is (-1.67, -0.052). Should we reject or retain $H_0: \mu = 0$?

What does this confidence interval suggest about the *p* value?

## Step 5. Draw a conclusion, based on the *p* value or confidence interval

We have the following results at the 5% significance level (equivalently, at the 95% confidence level, or with $\alpha$ = 0.05):

Approach | *p* value | 95% CI for $\mu_d$ | Conclusion re: $H_0$: $\mu_d$ = 0
:-------:|:---------------:|:----------------:|:------------------------------------:
t Test    | 0.048 | (-1.67, -0.007) | *p* < 0.05, so reject $H_0$
Wilcoxon  | 0.041 | (-2.0, -0.0004) | *p* < 0.05, so reject $H_0$
Bootstrap | < 0.05 | (-1.67, -0.052) | CI for $\mu$ excludes 0 so reject $H_0$

## Our Conclusions for the SBP in Diabetes Study

So, in this case, using any of these methods, we draw the same conclusion -- to reject $H_0$ at the 5% significance level and conclude as a result that:

a. there is a statistically significant difference between the population mean SBP of patients this year as compared to last year.
b. the population mean this year -- last year difference in SBP, which we have called $\mu_d$, is statistically significantly different from zero. 
c. In fact, the confidence intervals universally tell us that this population mean is negative -- SBP was (slightly) smaller this year than last year at the 95% confidence level.

## Paired Samples Study Designs

- Using a paired samples design means we carefully sample matched sets of subjects in pairs, so that the sampled subjects in each pair are as similar as possible, except for the exposure of interest. 
- Each observation in one exposure group is matched to a single observation in the other exposure group, so that taking paired differences is a rational thing to do. 
- Since every subject must be matched to exactly one subject in the other group, the sizes of the groups must be equal.

## Independent Samples Study Designs

- Independent samples designs do not impose such a matching, but instead sample two unrelated sets of subjects, where each group receives one of the two exposures. 
- The two groups of subjects are drawn independently from their separate populations of interest. 
- One obvious way to tell if we have an independent samples design is that this design does not require the sizes of the two exposure groups to be equal.

The best way to establish whether a study uses paired or independent samples is to look for the **link** between the two measurements that creates paired differences. 

- Deciding whether or not the samples are paired (matched) is something we do before we analyze the data.

## What if the Samples Aren't Paired?

In the `dm192` frame, we might also consider looking at a different kind of comparison, perhaps whether the average systolic blood pressure is larger in male or in female adults in NE Ohio living with diabetes.

```{r get the subset for male female comparison}
dm_second <- select(dm192, pt.id, sex, sbp)
summary(dm_second)
```

## Our comparison now is between females and males

```{r plot 1 to compare females and males, echo = FALSE}
ggplot(dm_second, aes(x = sex, y = sbp, fill = sex)) +
    geom_jitter(aes(color = sex), alpha = 0.75, width = 0.25) +
    geom_boxplot(notch = TRUE, alpha = 0.5) +
    coord_flip() +
    guides(fill = FALSE, color = FALSE) +
    theme_bw() + 
    labs(x = "", y = "Systolic Blood Pressure this year",
         title = "Independent Samples Comparison: SBP by Sex")
```

## Another Way to Picture Two Independent Samples

```{r histograms faceted for comparing sbp by sex, echo = FALSE}
ggplot(dm_second, aes(x = sbp, fill = sex)) +
  geom_histogram(bins = 12, col = "white") +
  facet_wrap(~ sex) +
  guides(fill = FALSE) + 
  labs(title = "Systolic Blood Pressure by Sex in 192 Patients with Diabetes")
```

## Numerical Summary for Two Independent Samples

```{r favstats comparing indep samples of sbp by sex, warning=FALSE}
by(dm_second$sbp, dm_second$sex, mosaic::favstats)
```

## Hypotheses Under Consideration

The hypotheses we are testing are:

- $H_0$: mean in population 1 = mean in population 2 + hypothesized difference $\Delta_0$ vs.
- $H_A$: mean in population 1 $\neq$ mean in population 2 + hypothesized difference $\Delta_0$, 

where $\Delta_0$ is almost always zero. An equivalent way to write this is:

- $H_0: \mu_1 = \mu_2 + \Delta_0$ vs. 
- $H_A: \mu_1 \neq \mu_2 + \Delta_0$ 

Yet another equally valid way to write this is: 

- $H_0: \mu_1 - \mu_2 = \Delta_0$ vs. 
- $H_A: \mu_1 - \mu_2 \neq \Delta_0$,

where, again $\Delta_0$ is almost always zero. 

## Testing Options for Independent Samples

1. Pooled t test or Indicator Variable Regression Model (t test assuming equal population variances)
2. Welch t test (t test without assuming equal population variances)
3. Wilcoxon-Mann-Whitney Rank Sum Test (non-parametric test not assuming populations are Normal)
4. Bootstrap confidence interval for the difference in population means

## Assumptions of the Pooled T test

The standard method for comparing population means based on two independent samples is based on the t distribution, and requires the following assumptions:

1.	[Independence] The samples for the two groups are drawn independently.
2.	[Random Samples] The samples for each of the groups are drawn at random from the populations of interest.
3.	[Normal Population] The two populations are each Normally distributed
4.	[Equal Variances] The population variances in the two groups being compared are the same, so we can obtain a pooled estimate of their joint variance.

## The Pooled Variances t test in R

Also referred to as the t test assuming equal population variances:

```{r t test 1 for sbp by sex}
t.test(dm_second$sbp ~ dm_second$sex, var.equal=TRUE)
```

## Assumptions of the Welch t test

The Welch test still requires:

1.	[Independence] The samples for the two groups are drawn independently.
2.	[Random Samples] The samples for each of the groups are drawn at random from the populations of interest.
3.	[Normal Population] The two populations are each Normally distributed

But it doesn't require:

4.	[Equal Variances] The population variances in the two groups being compared are the same.

Welch's t test is the default choice in R.

## Welch t test without assuming equal population variances

```{r t test 2 for sbp by sex}
t.test(dm_second$sbp ~ dm_second$sex)
```

## Assumptions of the Wilcoxon-Mann-Whitney Rank Sum Test

The Wilcoxon-Mann-Whitney Rank Sum test still requires:

1.	[Independence] The samples for the two groups are drawn independently.
2.	[Random Samples] The samples for each of the groups are drawn at random from the populations of interest.

But it doesn't require:

3.	[Normal Population] The two populations are each Normally distributed
4.	[Equal Variances] The population variances in the two groups being compared are the same.

It also doesn't really compare population means.

## Wilcoxon-Mann-Whitney Rank Sum Test

```{r rank sum test for sbp by sex}
wilcox.test(dm_second$sbp ~ dm_second$sex, conf.int = TRUE)
```

## The Bootstrap

This bootstrap approach to comparing population means using two independent samples still requires:

1.	[Independence] The samples for the two groups are drawn independently.
2.	[Random Samples] The samples for each of the groups are drawn at random from the populations of interest.

but does not require either of the other two assumptions:

3.	[Normal Population] The two populations are each Normally distributed
4.	[Equal Variances] The population variances in the two groups being compared are the same.

The bootstrap procedure I use in R was adapted from Frank Harrell and colleagues. http://biostat.mc.vanderbilt.edu/twiki/bin/view/Main/BootstrapMeansSoftware

## The `bootdif` function

The procedure requires the definition of a function, which I have adapted a bit, called `bootdif`, which is part of the `Love-boost.R` script on the web site, and is also part of this Markdown file.

As in our previous bootstrap procedures, we are sampling (with replacement) a series of many data sets (default: 2000).

- Here, we are building bootstrap samples based on the SBP levels in the two independent samples (M vs. F). 
- For each bootstrap sample, we are calculating a mean difference between the two groups (M vs. F).
- We then determine the 2.5th and 97.5th percentile of the resulting distribution of mean differences (for a 95% confidence interval).  

## Using the `bootdif` function to compare means based on independent samples

So, to compare systolic BP (our outcome) across the two levels of sex (our grouping factor) for the adult patients with diabetes in NE Ohio, run the following...

```{r run bootdif on SBP by sex, message=FALSE}
set.seed(4314); bootdif(dm_second$sbp, dm_second$sex)
```

Note that the two columns must be separated here with a comma rather than a tilde (`~`). 

This CI describes the male - female difference (i.e. the negative of the F-M difference used earlier) -- we can tell this by the listed sample mean difference. 

## Results for the SBP and Sex Study

Procedure     | 2-sided *p* value for $H_0: \mu_F = \mu_M$ | 95% CI for $\mu_F - \mu_M$
:-----------: | --------------------: | :------------------------:
Pooled t test | 0.463 | (-3.2, 7.0)
Welch t test  | 0.465 | (-3.2, 7.0)
Rank Sum test | 0.265 | (-2.0, 8.0)
Bootstrap CI  | *p* > 0.05 | (-2.9, 7.0)

What conclusions should we draw, at $\alpha$ = 0.05?

## A Few Comments on Significance

- **A significant effect is not necessarily the same thing as an interesting effect.**  For example, results calculated from large samples are nearly always "significant" even when the effects are quite small in magnitude.  Before doing a test, always ask if the effect is large enough to be of any practical interest.  If not, why do the test?

- **A non-significant effect is not necessarily the same thing as no difference.**  A large effect of real practical interest may still produce a non-significant result simply because the sample is too small.

- **There are assumptions behind all statistical inferences.** Checking assumptions is crucial to validating the inference made by any test or confidence interval.


## On Reporting *p* Values

When reporting a *p* value and no rounding rules are in place from the lead author/journal/source for publication, follow these conventions...

1. Use an italicized, lower-case *p* to specify the *p* value. Don't use *p* for anything else.
2. For *p* values above 0.10, round to two decimal places, at most. 
3. For *p* values near $\alpha$, include only enough decimal places to clarify the reject/retain decision. 
4. For very small *p* values, always report either *p* < 0.0001 or even just *p* < 0.001, rather than specifying the result in scientific notation, or, worse, as $p = 0$ which is glaringly inappropriate.
5. Report *p* values above 0.99 as *p* > 0.99, rather than *p* = 1.

## From George Cobb - on why *p* values deserve to be re-evaluated

The **idea** of a p-value as one possible summary of evidence

morphed into a

- **rule** for authors:  reject the null hypothesis if p < .05.

## From George Cobb - on why *p* values deserve to be re-evaluated

The **idea** of a p-value as one possible summary of evidence

morphed into a

- **rule** for authors:  reject the null hypothesis if p < .05,

which morphed into a

- **rule** for editors:  reject the submitted article if p > .05.

## From George Cobb - on why *p* values deserve to be re-evaluated

The **idea** of a p-value as one possible summary of evidence

morphed into a

- **rule** for authors:  reject the null hypothesis if p < .05,

which morphed into a

- **rule** for editors:  reject the submitted article if p > .05,

which morphed into a

- **rule** for journals:  reject all articles that report p-values\footnote{http://www.nature.com/news/psychology-journal-bans-p-values-1.17001 describes the recent banning of null hypothesis significance testing by {\it Basic and Applied Psychology}.} 

## From George Cobb - on why *p* values deserve to be re-evaluated

The **idea** of a p-value as one possible summary of evidence

morphed into a

- **rule** for authors:  reject the null hypothesis if p < .05, which morphed into a

- **rule** for editors:  reject the submitted article if p > .05, which morphed into a

- **rule** for journals:  reject all articles that report p-values. 

Bottom line:  **Reject rules.  Ideas matter.**

*Posted to an American Statistical Association message board Oct 14 2015*
